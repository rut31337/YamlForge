---
# GPU Workloads Example
# Demonstrates GPU instance types across all cloud providers and cost optimization

yamlforge:
  cloud_workspace:
    name: "gpu-ai-ml-workloads"
    description: "Multi-cloud GPU deployment for AI/ML workloads"

  # SSH key configuration
  ssh_keys:
    default: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC7... gpu-workloads@company.com"

  # Security groups for GPU workloads
  security_groups:
    ml-gpu-sg:
      description: "Machine Learning GPU instances"
      rules:
        - protocol: tcp
          port: 22
          source: "10.0.0.0/8"  # Internal SSH access
        - protocol: tcp
          port: 8888
          source: "10.0.0.0/8"  # Jupyter Notebook
        - protocol: tcp
          port: 6006
          source: "10.0.0.0/8"  # TensorBoard

  instances:
    # Generic GPU size-based instances (cross-cloud compatibility)
    - name: "ml-dev-small"
      provider: "aws"
      size: "gpu_small"
      region: "us-east-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "development"
        gpu_tier: "small"
        framework: "tensorflow"

    - name: "ml-training-medium"
      provider: "azure" 
      size: "gpu_medium"
      region: "eastus"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "training"
        gpu_tier: "medium"
        framework: "pytorch"

    - name: "ml-inference-large"
      provider: "gcp"
      size: "gpu_large"
      region: "us-central1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "inference"
        gpu_tier: "large"
        framework: "huggingface"

    - name: "distributed-training"
      provider: "ibm_vpc"
      size: "gpu_xlarge"
      region: "us-south"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "distributed_training"
        gpu_tier: "xlarge"
        framework: "pytorch_lightning"

    # Hardware specification-based GPU instances (exact requirements)
    - name: "custom-gpu-1"
      provider: "cheapest"
      cores: 8
      memory: 32768  # 32GB
      gpu_count: 1
      region: "us-east-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "custom_training"
        optimization: "cost_optimized"
        requirement: "8cpu-32gb-1gpu"

    - name: "custom-gpu-2"
      provider: "cheapest"
      cores: 16
      memory: 65536  # 64GB
      gpu_count: 2
      region: "us-west-2"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "multi_gpu_training"
        optimization: "cost_optimized"
        requirement: "16cpu-64gb-2gpu"

    - name: "high-memory-gpu"
      provider: "cheapest"
      cores: 32
      memory: 131072  # 128GB
      gpu_count: 4
      region: "eu-west-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "large_model_training"
        optimization: "cost_optimized"
        requirement: "32cpu-128gb-4gpu"

    # Provider-specific optimized instances
    - name: "aws-p4d-flagship"
      provider: "aws"
      instance_type: "p4d.24xlarge"  # Direct instance type
      region: "us-east-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "flagship_training"
        gpu_type: "nvidia_a100"
        provider_optimized: "true"

    - name: "azure-nd-series"
      provider: "azure"
      instance_type: "Standard_ND96amsr_A100_v4"  # Direct instance type
      region: "eastus"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "enterprise_training"
        gpu_type: "nvidia_a100"
        provider_optimized: "true"

    - name: "gcp-a2-mega"
      provider: "gcp"
      instance_type: "a2-megagpu-16g"  # Direct instance type
      region: "us-central1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "research_cluster"
        gpu_type: "nvidia_a100"
        provider_optimized: "true"

    - name: "ibm-gx3-high-end"
      provider: "ibm_vpc"
      instance_type: "gx3-128x1024x4l40s"  # Direct instance type
      region: "us-south"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "hpc_workload"
        gpu_type: "nvidia_l40s"
        provider_optimized: "true"

    # Cost-optimized mixed deployment
    - name: "budget-gpu-dev-1"
      provider: "cheapest"
      cores: 4
      memory: 16384  # 16GB
      gpu_count: 1
      region: "us-central-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "budget_development"
        optimization: "cost_first"
        tier: "entry_level"

    - name: "budget-gpu-dev-2"
      provider: "cheapest"
      cores: 6
      memory: 24576  # 24GB
      gpu_count: 1
      region: "eu-central-1"
      image: "RHEL9-latest"
      security_groups: ["ml-gpu-sg"]
      tags:
        workload_type: "budget_development"
        optimization: "cost_first"
        tier: "mid_range"

  # Global tags
  tags:
    project: "ai-ml-infrastructure"
    managed_by: "yamlforge"
    workload_category: "gpu_compute"
    cost_center: "research_and_development" 